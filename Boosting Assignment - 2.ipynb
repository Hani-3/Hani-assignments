{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d86af834-49c8-4d2c-a1df-fd8a3c26847f",
   "metadata": {},
   "source": [
    "**Q1. What is Gradient Boosting Regression?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f57357-ce97-4163-95cc-e473f800b22f",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It's a variant of boosting algorithms that builds an ensemble of weak regression models, typically decision trees, in a sequential manner. The key idea behind Gradient Boosting Regression is to fit new models to the residuals (the differences between the observed and predicted values) of the previous models, iteratively reducing the errors.\n",
    "\n",
    "Gradient Boosting Regression is called \"gradient\" because it minimizes the loss function of the model using gradient descent optimization. It iteratively improves the model by moving in the direction of the negative gradient of the loss function with respect to the predictions.\n",
    "\n",
    "This technique is powerful because it can capture complex relationships in the data and produce highly accurate predictions. However, it can also be prone to overfitting if not properly regularized or if the number of iterations is too high. Regularization techniques such as limiting the tree depth or using shrinkage (learning rate) can help prevent overfitting and improve the generalization performance of Gradient Boosting Regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5716217-c110-47b8-a830-3835d1d4781c",
   "metadata": {},
   "source": [
    "**Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3826fff5-d600-4238-a49f-04586846b7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.005590233992064395\n",
      "R-squared: 0.9999960756206341\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize model with mean of target variable\n",
    "        self.init_pred = np.mean(y)\n",
    "        pred = np.full_like(y, self.init_pred)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute negative gradient (residuals)\n",
    "            residuals = y - pred\n",
    "\n",
    "            # Fit regression tree to negative gradient\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update model with tree predictions\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = np.full(X.shape[0], self.init_pred)\n",
    "        for tree in self.trees:\n",
    "            pred += self.learning_rate * tree.predict(X)\n",
    "        return pred\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum((y_true - y_pred)**2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true))**2)\n",
    "    return 1 - (ss_res / ss_tot)\n",
    "\n",
    "# Example usage\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Fit gradient boosting model\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X, y)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = gb.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r_squared(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12814d-da67-448c-981b-0a1680ff4bda",
   "metadata": {},
   "source": [
    "**Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "535b7e44-6b00-42ee-bbf3-ebc3514c9f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'n_estimators': 150, 'max_depth': 3, 'learning_rate': 0.2}\n",
      "Best MSE: 10.403069276033282\n",
      "Mean Squared Error: 0.0019152106331373864\n",
      "R-squared: 0.9999986555101091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [2, 3, 4]\n",
    "}\n",
    "\n",
    "# Initialize gradient boosting regressor\n",
    "gb = GradientBoostingRegressor()\n",
    "\n",
    "# Grid search\n",
    "random_search = RandomizedSearchCV(estimator=GradientBoostingRegressor(), param_distributions=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best MSE:\", -best_score)\n",
    "\n",
    "y_pred = random_search.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r_squared(y, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R-squared: {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a819c-26ff-4d09-ab26-2a698185c809",
   "metadata": {},
   "source": [
    "**Q4. What is a weak learner in Gradient Boosting?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9c393a-ba13-45d4-8819-d3ccd20e823f",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a base model that is only slightly better than random guessing for the given problem. These weak learners are typically simple models, such as decision trees with shallow depth (often referred to as \"stumps\") or linear regression models. The term \"weak\" does not imply that the model is inherently poor, but rather that it's not sufficiently expressive to solve the problem on its own.\n",
    "\n",
    "The concept of using weak learners in Gradient Boosting is central to its operation. In the boosting process, weak learners are sequentially added to the ensemble, with each subsequent learner attempting to correct the errors made by the previous ones. By combining many weak learners into an ensemble, Gradient Boosting can create a strong learner that achieves high predictive performance.\n",
    "\n",
    "The key idea behind using weak learners is that even though each individual learner may have limited predictive power, the ensemble can effectively capture complex patterns in the data by focusing on the areas where previous learners have performed poorly. This iterative approach of sequentially fitting weak learners and adjusting the ensemble's predictions gradually improves the model's overall performance.\n",
    "\n",
    "The most common weak learner used in Gradient Boosting frameworks like XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor/GradientBoostingClassifier is a decision tree with shallow depth. These decision trees are usually constrained to have a small number of nodes (e.g., one or two splits), which prevents them from capturing complex interactions in the data and encourages them to focus on the most informative features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e31ff-b309-413f-9d58-4d7c7b23190e",
   "metadata": {},
   "source": [
    "**Q5. What is the intuition behind the Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d166c2-b861-4a1d-9b03-219b150fe1cf",
   "metadata": {},
   "source": [
    "- Ensemble Learning: Gradient Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong learner. Each weak learner typically performs slightly better than random guessing on the task at hand.\n",
    "- Sequential Learning: Unlike bagging techniques like Random Forest, which train multiple models independently and then combine their predictions, Gradient Boosting trains weak learners sequentially. Each new weak learner is trained to correct the errors made by the existing ensemble.\n",
    "- Gradient Descent Optimization: The \"gradient\" in Gradient Boosting refers to the optimization process used to minimize a loss function. In each iteration, the algorithm calculates the gradient of the loss function with respect to the ensemble's predictions. It then trains a new weak learner to minimize the loss by following the direction of the negative gradient.\n",
    "- Additive Modeling: Gradient Boosting builds the ensemble model in an additive manner, where each weak learner is added to the ensemble to improve the overall predictions. At each step, the new weak learner is trained to predict the residual errors of the current ensemble.\n",
    "- Shrinkage (or Learning Rate): To prevent overfitting and improve generalization, Gradient Boosting introduces a shrinkage parameter (also known as the learning rate). This parameter scales the contribution of each new weak learner to the ensemble. A smaller learning rate slows down the learning process, allowing for more precise adjustments to the ensemble's predictions.\n",
    "- Regularization: Gradient Boosting often includes regularization techniques, such as limiting the depth of individual trees or adding constraints on the complexity of the weak learners. These regularization techniques help prevent overfitting and improve the generalization ability of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e207bc-34a8-4d8e-b262-737ae3ed3308",
   "metadata": {},
   "source": [
    "**Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaafa87-1b76-40e6-a458-96fd8cfc2161",
   "metadata": {},
   "source": [
    "1. Start with a Basic Learner: Train a weak learner, like a shallow decision tree, on the original data. This initial model provides a starting point for improvement.\n",
    "2. Calculate the Errors: Analyze the predictions of the first learner and calculate the errors (residuals) for each data point. These residuals represent the difference between the actual values and the model's predictions.\n",
    "3. Train the Next Learner: Train a new weak learner on these residuals. This learner specifically tries to capture the patterns in the errors that the first model missed. The goal is to improve upon the initial predictions by focusing on the areas where the first model struggled.\n",
    "4. Combine Predictions:  Here comes the boosting part. The predictions from all the weak learners are combined, typically through an additive approach. Each weak learner's contribution is often weighted to control its influence on the final prediction.\n",
    "5. Repeat and Improve:  The entire process (steps 2-4) is repeated for multiple iterations. With each iteration, a new weak learner is trained on the residuals of the previous ensemble, focusing on the remaining errors. This way, the ensemble progressively improves its ability to handle the complexities in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb66c1df-5ef1-4a01-93e0-19179a43817c",
   "metadata": {},
   "source": [
    "**Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74134e2-422d-4fcf-982b-953a4a696b84",
   "metadata": {},
   "source": [
    "1. Loss Function: Start by defining a loss function that quantifies the difference between the model's predictions and the true target values. Common loss functions for regression tasks include mean squared error (MSE) and mean absolute error (MAE), while for classification tasks, cross-entropy loss or exponential loss are often used.\n",
    "2. Gradient Descent: Understand the concept of gradient descent, which is an optimization technique used to minimize the loss function by iteratively adjusting the model parameters in the direction of the negative gradient of the loss function. In Gradient Boosting, the negative gradient represents the direction of steepest descent towards the minimum of the loss function.\n",
    "3. Weak Learners: Introduce the concept of weak learners, which are simple models that perform slightly better than random guessing on the task at hand. In Gradient Boosting, decision trees with shallow depth are commonly used as weak learners due to their simplicity and flexibility.\n",
    "4. Additive Modeling: Explain how Gradient Boosting builds an ensemble of weak learners in an additive manner, with each new weak learner trained to correct the errors made by the existing ensemble. The final prediction is obtained by summing the predictions of all weak learners in the ensemble.\n",
    "5. Gradient Boosting Algorithm: Develop the step-by-step algorithm for Gradient Boosting, which involves iteratively fitting weak learners to the negative gradients (pseudo-residuals) of the loss function and updating the ensemble predictions by adding a fraction of the weak learner's predictions. The learning rate parameter controls the contribution of each weak learner to the ensemble.\n",
    "6. Regularization: Discuss the importance of regularization techniques in Gradient Boosting to prevent overfitting and improve the generalization ability of the model. Common regularization techniques include limiting the depth of individual trees, adding constraints on the complexity of weak learners, and using a smaller learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f474de-c00d-47db-a5ea-6261f7baf995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
