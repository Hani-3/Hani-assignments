{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fa8d9f-939b-47ec-b8f9-93c78ef69256",
   "metadata": {},
   "source": [
    "**Q1. What is Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5184f0b-2969-46ce-b221-dad867c76910",
   "metadata": {},
   "source": [
    "Bayes' theorem is a method used in probability theory to determine the probability of an event (A) occurring, given that another event (B) has already happened. In simpler terms, it allows you to update the likelihood of something (A) being true based on new evidence (B)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3929f731-ef12-4592-a20e-d43652c82102",
   "metadata": {},
   "source": [
    "**Q2. What is the formula for Bayes' theorem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c2b6b-00ab-4dc5-9e31-5addf8dd5ea0",
   "metadata": {},
   "source": [
    "$ P(A|B) = \\frac{P(B|A) \\times P(A)}{P(B)} $\n",
    "\n",
    "Where:\n",
    "- $ P(A|B) $ is the probability of hypothesis $ A $ given evidence $ B $ (called the posterior probability).\n",
    "- $ P(B|A) $ is the probability of evidence $ B $ given hypothesis $ A $ (called the likelihood).\n",
    "- $ P(A) $ is the prior probability of hypothesis $ A $ before considering evidence $ B $.\n",
    "- $ P(B) $ is the probability of the evidence $ B $, also called the marginal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d35d6d-9cc9-4b48-a89b-879caa481e8d",
   "metadata": {},
   "source": [
    "**Q3. How is Bayes' theorem used in practice?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab80b1a-93c5-48db-b08f-94d6d57718c3",
   "metadata": {},
   "source": [
    "- Medical Diagnosis: Bayes' theorem is used in medical diagnosis to update the probability of a disease given certain symptoms or test results. For example, it can help determine the probability of a patient having a particular disease based on the symptoms they exhibit and the results of diagnostic tests.\n",
    "\n",
    "- Spam Filtering: In email spam filtering, Bayes' theorem is used to classify emails as spam or non-spam based on the occurrence of certain words or phrases in the email content. This approach, known as Bayesian spam filtering, continuously updates the probability of an email being spam or not based on the incoming data.\n",
    "\n",
    "- Machine Learning: Bayes' theorem serves as the foundation for various machine learning algorithms, especially in probabilistic modeling and inference. Bayesian methods are used for tasks such as classification, regression, clustering, and anomaly detection. Bayesian networks, which represent probabilistic relationships among variables, are also widely used in machine learning.\n",
    "\n",
    "- Weather Forecasting: Bayes' theorem is employed in weather forecasting to update the probability of different weather conditions based on new observational data. By incorporating data from weather sensors, satellite imagery, and historical weather patterns, meteorologists can improve the accuracy of their forecasts.\n",
    "\n",
    "- Document Classification: Bayes' theorem is utilized in document classification tasks, such as categorizing news articles, scientific papers, or legal documents into predefined categories. This is commonly done using algorithms like Naive Bayes, which assumes independence between features (words) in the documents.\n",
    "\n",
    "- Risk Assessment: Bayes' theorem is applied in risk assessment and decision-making processes, such as insurance underwriting, financial risk management, and cybersecurity. By updating the probabilities of different outcomes based on new information or events, decision-makers can make more informed and efficient choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39a8129-f12e-48b2-bec3-23560c1bd0d1",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7111a54-95ba-4691-8047-95e3adebb388",
   "metadata": {},
   "source": [
    "Conditional probability deals with the probability of event A happening given that event B has already occurred. This is very similar to what Bayes' theorem helps us calculate (P(A|B)).\n",
    "\n",
    "However, conditional probability stops at just this calculation, whereas Bayes' theorem goes a step further. It allows us to use the initial probability of event A (P(A)) and the probability of event B occurring given A (P(B|A)) to calculate the updated probability of A given B (P(A|B)). \n",
    "\n",
    "In essence, Bayes' theorem incorporates conditional probability as a key part of its formula. It lets you revise the probability of something (A) happening in light of new information (B) by considering both the likelihood of B given A and the prior probability of A itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b215b-b90c-484c-b8dc-7a3b2066dfe2",
   "metadata": {},
   "source": [
    "**Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3099e44b-07bd-4b39-93d4-1d83e17870f4",
   "metadata": {},
   "source": [
    "- Gaussian Naive Bayes: This variant is suitable for continuous data that approximately follows a Gaussian (normal) distribution. If your features are continuous and you can reasonably assume they are normally distributed within each class, Gaussian Naive Bayes is a good choice.\n",
    "\n",
    "- Multinomial Naive Bayes: This variant is commonly used for text classification tasks where the features (e.g., word counts or frequencies) are discrete and represent the occurrence of words or tokens. It's suitable for problems involving categorical features with multiple levels.\n",
    "\n",
    "- Bernoulli Naive Bayes: Similar to Multinomial Naive Bayes, this variant is also used for binary or categorical features. It assumes that features are binary-valued (e.g., presence or absence of a feature). It's often used in document classification tasks where the presence or absence of words is considered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce25e23-d8ff-473d-8512-9c86aaab145e",
   "metadata": {},
   "source": [
    "**Q6. Assignment:     \n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:         \n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4             \n",
    "A 3 3 4 4 3 3 3               \n",
    "B 2 2 1 2 2 2 3             \n",
    "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712b8f20-0425-4788-92f4-819a62128b30",
   "metadata": {},
   "source": [
    "To classify the new instance using Naive Bayes, we need to calculate the probability of each class given the features \\( X_1 = 3 \\) and \\( X_2 = 4 \\) using Bayes' theorem. \n",
    "\n",
    "Given that we have equal prior probabilities for each class, we can simplify the computation by ignoring the denominator (marginal likelihood) in Bayes' theorem, as it is the same for both classes. Thus, we only need to compare the numerators for each class.\n",
    "\n",
    "Let's denote $ P(A) $ and $ P(B) $ as the prior probabilities of classes A and B, respectively, and $ P(X_1=3|A) $, $ P(X_2=4|A) $, $ P(X_1=3|B) $, $ P(X_2=4|B) $ as the conditional probabilities of features $ X_1 = 3 $ and $ X_2 = 4 $ given classes A and B.\n",
    "\n",
    "Given the table provided:\n",
    "\n",
    "$ P(X_1=3|A) = \\frac{4}{13} $           \n",
    "$ P(X_2=4|A) = \\frac{3}{13} $           \n",
    "$ P(X_1=3|B) = \\frac{1}{9} $            \n",
    "$ P(X_2=4|B) = \\frac{3}{9} $                \n",
    " \n",
    "Since we have equal prior probabilities for each class, $ P(A) = P(B) = 0.5 $.          \n",
    "\n",
    "Now, we can calculate the numerator of Bayes' theorem for each class:         \n",
    "\n",
    "For class A:           \n",
    "$ P(A|X_1=3, X_2=4) = P(X_1=3|A) \\times P(X_2=4|A) \\times P(A) = \\frac{4}{13} \\times \\frac{3}{13} \\times 0.5 = \\frac{12}{338} $              \n",
    "\n",
    "For class B:       \n",
    "$ P(B|X_1=3, X_2=4) = P(X_1=3|B) \\times P(X_2=4|B) \\times P(B) = \\frac{1}{9} \\times \\frac{3}{9} \\times 0.5 = \\frac{3}{162} $               \n",
    "\n",
    "Comparing the numerators, $ P(A|X_1=3, X_2=4) < P(B|X_1=3, X_2=4) $, therefore, Naive Bayes would predict the new instance to belong to class B.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ac4928-61ae-4273-8b03-612c09d65857",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
