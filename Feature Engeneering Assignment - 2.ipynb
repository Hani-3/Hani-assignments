{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "590ba3cf-3be2-467a-9885-c00094a24aff",
   "metadata": {},
   "source": [
    "**Q1. What is Min-Max scaling, and how it is used in data preprocessing? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637440cc-cae6-4afa-9221-88a73bfdcc1a",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data preprocessing technique used to rescale numerical features to a fixed range, typically between 0 and 1. It works by subtracting the minimum value from each observation and then dividing by the range (the difference between the maximum and minimum values).\n",
    "\n",
    "The purpose of Min-Max scaling is to ensure that all features have the same scale, which can be crucial for certain machine learning algorithms, especially those based on distance calculations or optimization methods that are sensitive to the scale of the features. Min-Max scaling is quite straightforward to implement and can be beneficial when the distribution of the data is relatively uniform and does not contain outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4129c7b2-eb71-4d18-98ac-275f5db659c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # import required library\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]] # provide data\n",
    "scaler = MinMaxScaler() \n",
    "scaler.fit(data) # fit data in min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd7d853d-5ddd-4405-9457-cb4ac744e7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.transform(data)) # to print transformed data within range 0 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c302a-0929-45fe-b5b6-3d59da9ad886",
   "metadata": {},
   "source": [
    "**Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c4c7b1-f6c5-43a5-b811-643210be231b",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or unit normalization, is a feature scaling method used to scale the values of individual features to have a unit norm (length) in a vector space. This technique is particularly useful when the direction of the data points is more important than their actual magnitude.\n",
    "\n",
    "The main difference between Unit Vector scaling and Min-Max scaling is that Unit Vector scaling does not constrain the scaled values to a specific range like [0, 1]. Instead, it focuses on the direction of the data points, ensuring that they all lie on the unit hypersphere (a sphere of radius 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a27bf042-9cf4-4bc8-be47-a80afb3742b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.66666667,  0.33333333,  0.66666667],\n",
       "       [-0.70710678,  0.        ,  0.70710678]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize #provide important library\n",
    "X = [[-2, 1, 2], [-1, 0, 1]] # provide data\n",
    "normalize(X) #apply normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b267c0-6a35-4ff9-abb7-95d261778e91",
   "metadata": {},
   "source": [
    "**Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b4fe6-311c-40a0-93fa-07b010105352",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction in datasets with many variables, while preserving most of the original information. It works by transforming the original variables into a new set of orthogonal (uncorrelated) variables called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first component explaining the maximum variance, the second explaining the second most, and so on.\n",
    "\n",
    "PCA is used to reduce the dimensionality of the dataset by projecting it onto a lower-dimensional subspace while retaining as much of the original variation as possible. This reduction in dimensionality can be particularly useful for visualizing high-dimensional data, speeding up machine learning algorithms, and reducing noise and redundancy in the data.\n",
    "\n",
    "Example:   \n",
    "Suppose you have a dataset containing information about houses, including features like square footage, number of bedrooms, number of bathrooms, etc. You want to reduce the dimensionality of this dataset using PCA.    \n",
    "After standardizing the data and calculating the covariance matrix, you find that the first principal component explains 80% of the variance, the second principal component explains 15%, and the remaining components explain only 5% combined.   \n",
    "In this case, you might choose to retain only the first two principal components, which capture 95% of the variance in the data. You can then transform the original data into a new dataset consisting of just these two principal components, effectively reducing the dimensionality of the dataset while preserving most of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe4498-2e9b-454c-909a-f2e0b7cf325d",
   "metadata": {},
   "source": [
    "**Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0683be9-c568-416a-b81b-052bc5360222",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a technique commonly used for feature extraction in machine learning and data analysis. Feature extraction is the process of reducing the number of features (variables) in a dataset by creating new features that capture the most important information in the original features. PCA achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components.\n",
    "\n",
    "The relationship between PCA and feature extraction lies in the fact that PCA is a method for extracting a smaller set of features from a larger set of features while preserving the most important information. It does this by identifying the directions (principal components) in which the data varies the most and projecting the data onto these directions.\n",
    "\n",
    "How PCA can be used for feature extraction:\n",
    "\n",
    "1. **Standardize the data**: If the features in the dataset have different scales, standardize them to ensure that all features contribute equally to the analysis.\n",
    "\n",
    "2. **Apply PCA**: Calculate the covariance matrix of the standardized data and compute the eigenvectors and eigenvalues. Sort the eigenvectors in descending order of eigenvalues to identify the principal components.\n",
    "\n",
    "3. **Select the number of components**: Determine the number of principal components to retain based on the explained variance. You can choose to retain a certain number of components that explain a high percentage of the total variance (e.g., 95%).\n",
    "\n",
    "4. **Transform the data**: Project the original data onto the selected principal components to create a new dataset with reduced dimensionality.\n",
    "\n",
    "Example to illustrate this concept:\n",
    "\n",
    "Suppose you have a dataset containing images of handwritten digits, where each image is represented by a matrix of pixel values. Each pixel can be considered as a feature, resulting in a high-dimensional dataset. You want to reduce the dimensionality of this dataset for use in a machine learning algorithm.\n",
    "\n",
    "By applying PCA to this dataset, you can extract a smaller set of features (principal components) that capture the most important information in the images. For example, you might find that the first few principal components correspond to patterns that represent the shape of the digits (e.g., straight lines for digits like 1 or 7, curves for digits like 0 or 6). By retaining only these principal components, you can reduce the dimensionality of the dataset while still preserving the essential characteristics of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da1888-646b-4524-8941-21f7fbda4666",
   "metadata": {},
   "source": [
    "**Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef50535-af24-4a6c-906b-71cf1bb8e6ac",
   "metadata": {},
   "source": [
    "1. Identify relevant features for scaling:\n",
    "\n",
    "- Price: This feature likely has a positive skew (more data points towards the lower end). Min-Max scaling can be beneficial here.\n",
    "- Rating: Rating data is typically bounded between a minimum and maximum value (e.g., 1 to 5 stars). Scaling might not be strictly necessary unless the distribution is highly skewed or the chosen recommendation algorithm is sensitive to scales.\n",
    "- Delivery Time: Delivery time can vary significantly depending on factors like distance and traffic. Scaling is generally recommended for this feature.\n",
    "\n",
    "2. Apply Min-Max scaling for each feature independently:\n",
    "\n",
    "For each feature you decide to scale (price and delivery time in this case), follow these steps:\n",
    "- Calculate the minimum (min_value) and maximum (max_value) values for the chosen feature across the entire dataset.\n",
    "- Iterate through each data point in the feature:\n",
    "    - Subtract the minimum value (min_value) from the data point.\n",
    "    - Divide the resulting value by the difference between the maximum and minimum values (max_value - min_value).\n",
    "    - Replace the original data point with the scaled value (between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c43d5c1-4dd3-4137-8218-5fd44285e718",
   "metadata": {},
   "source": [
    "**Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08365e90-02c7-4a4d-8844-b1f4d9406d23",
   "metadata": {},
   "source": [
    "1. **Data Preprocessing**:\n",
    "- Ensure that the dataset is cleaned and preprocessed, including handling missing values, outliers, and ensuring that all features are on a similar scale.\n",
    "- Standardize the features if they have different scales. PCA works best when the features are standardized, i.e., have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "- Compute the covariance matrix of the standardized dataset. The covariance matrix measures the relationships between pairs of features.\n",
    "- Compute the eigenvectors and eigenvalues of the covariance matrix. These represent the directions and magnitudes of the principal components.\n",
    "- Sort the eigenvectors by their corresponding eigenvalues in descending order. The eigenvectors with the largest eigenvalues contain the most information about the dataset.\n",
    "- Select the top \\( k \\) eigenvectors corresponding to the largest eigenvalues to form the new subspace. \\( k \\) is the desired number of principal components, which determines the dimensionality reduction.\n",
    "- Project the original data onto the selected principal components to obtain the lower-dimensional representation of the dataset.\n",
    "\n",
    "3. **Choosing the Number of Principal Components**:\n",
    "- Decide on the number of principal components to retain based on the explained variance ratio. The explained variance ratio tells you the proportion of variance in the original dataset that is explained by each principal component.\n",
    "- You can plot the cumulative explained variance ratio and choose the number of principal components that capture a significant portion of the variance, such as 95% or 99%.\n",
    "\n",
    "4. **Dimensionality Reduction**:\n",
    "- Once you've selected the desired number of principal components, transform the original dataset into the reduced-dimensional space by projecting it onto the selected principal components.\n",
    "\n",
    "5. **Modeling**:\n",
    "- Use the reduced-dimensional dataset as input to your stock price prediction model. You can use various machine learning algorithms such as regression, time series analysis, or deep learning techniques.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defab175-9a5d-48e4-9148-7ba7f1f8aa88",
   "metadata": {},
   "source": [
    "**Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2b9295b-31cc-427e-b09b-e795b24f823b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler(feature_range=(-1, 1))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler(feature_range=(-1, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler #required library\n",
    "data = [[1],[5],[10],[15],[20]] # given dataset\n",
    "min_max = MinMaxScaler(feature_range=(-1, 1)) #to create min-max scaler in range -1 to 1\n",
    "min_max.fit(data) #to fit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa53d658-7b35-442a-b865-b0b83e733ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max.transform(data) # to transform data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a18c342-a327-41b3-9301-228f2188aeda",
   "metadata": {},
   "source": [
    "**Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81510ff1-503c-4dbe-8f69-f53ab0224698",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], we need to follow these steps:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "- Standardize the features if they have different scales. PCA works best when the features are standardized, i.e., have a mean of 0 and a standard deviation of 1.\n",
    "- If necessary, encode categorical features like gender using one-hot encoding.\n",
    "\n",
    "2. **Apply PCA**:\n",
    "- Compute the covariance matrix of the standardized dataset.\n",
    "- Compute the eigenvectors and eigenvalues of the covariance matrix.\n",
    "- Sort the eigenvectors by their corresponding eigenvalues in descending order.\n",
    "- Select the top \\( k \\) eigenvectors corresponding to the largest eigenvalues to form the new subspace.\n",
    "- Project the original data onto the selected principal components to obtain the lower-dimensional representation of the dataset.\n",
    "\n",
    "3. **Choosing the Number of Principal Components**:\n",
    "- Decide on the number of principal components to retain based on the explained variance ratio.\n",
    "- Plot the cumulative explained variance ratio and choose the number of principal components that capture a significant portion of the variance, such as 95% or 99%.\n",
    "- Alternatively, you can use domain knowledge or conduct cross-validation to determine the optimal number of principal components.\n",
    "\n",
    "Principal components to retain for this specific dataset:\n",
    "\n",
    "- Since we have 5 features, we could theoretically have up to 5 principal components.\n",
    "- However, we typically aim to retain a smaller number of principal components that capture most of the variance in the dataset.\n",
    "- To determine the optimal number of principal components, we can examine the explained variance ratio.\n",
    "- We may choose to retain enough principal components to capture a significant portion of the variance, such as 95% or 99%.\n",
    "- Additionally, we may consider the trade-off between dimensionality reduction and preserving information. Retaining fewer principal components reduces the dimensionality but may lose some information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7479bae-d1e0-4175-9ac9-f4a00ff23f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
