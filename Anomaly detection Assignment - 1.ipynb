{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee149af2-3fcd-4142-ad06-be802a63591e",
   "metadata": {},
   "source": [
    "**Q1. What is anomaly detection and what is its purpose?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e350b-7437-4550-9065-133ec75cce80",
   "metadata": {},
   "source": [
    "Anomaly detection, also known as outlier detection, is a technique in data analysis that focuses on identifying unusual patterns, data points, or events that deviate significantly from the established norm. It's essentially finding the \"odd ones out\" in a dataset.\n",
    "\n",
    "Purpose of Anomaly Detection:    \n",
    "The primary purpose of anomaly detection is to uncover these anomalies which can signal various important things:\n",
    "- Suspicious activity: In finance, it can help detect fraudulent transactions. In cybersecurity, it can identify potential network intrusions.\n",
    "- Equipment malfunction: In manufacturing, it can be used to predict equipment failure before it happens.\n",
    "- Medical conditions: In healthcare, it can help identify abnormal patient readings that might indicate a health issue.\n",
    "- Unexpected trends: Anomaly detection can reveal new and unforeseen patterns in data, leading to new discoveries and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a8bfe2-fcb9-4aba-9437-8f07f98f4a15",
   "metadata": {},
   "source": [
    "**Q2. What are the key challenges in anomaly detection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e617f7de-382e-420c-9273-5a2ee54c83a8",
   "metadata": {},
   "source": [
    "- High Dimensionality: Analyzing datasets with many features can be complex and computationally intensive, making it harder to identify anomalies.\n",
    "- Data Imbalance: Anomalies are often rare compared to normal instances, leading to imbalanced datasets that challenge detection methods.\n",
    "- Dynamic Data: In many applications, data is not static but evolves over time, requiring adaptive detection methods.\n",
    "- Noise: Differentiating between true anomalies and noise (random fluctuations in the data) can be difficult.\n",
    "- Lack of Labeled Data: Obtaining labeled datasets with known anomalies is often challenging, complicating the training of supervised models.\n",
    "- Contextual Anomalies: Some anomalies are only identifiable within a specific context, making them harder to detect without additional contextual information.\n",
    "- Scalability: Detecting anomalies in large-scale data, such as real-time streaming data, requires scalable and efficient algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189d1f0-b436-4595-9d7c-fbf06da7d79e",
   "metadata": {},
   "source": [
    "**Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e32fe-63f4-49b5-9f84-2efccaa9c5c7",
   "metadata": {},
   "source": [
    "Supervised Anomaly Detection:\n",
    "- Training Data: Requires a labeled dataset where instances are marked as normal or anomalous.\n",
    "- Learning Process: Models learn to distinguish between normal and anomalous instances based on the provided labels.\n",
    "- Examples: Techniques include supervised machine learning algorithms like decision trees, support vector machines, and neural networks trained on labeled data.\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "- Training Data: Does not require labeled data; the algorithm identifies patterns and deviations based on the inherent structure of the data.\n",
    "- Learning Process: Models infer normal behavior from the data and flag deviations as anomalies.\n",
    "- Examples: Techniques include clustering algorithms (e.g., k-means), principal component analysis (PCA), and distance-based methods that do not rely on labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2844c880-cc6e-46d0-b9f6-c16f5b7a766a",
   "metadata": {},
   "source": [
    "**Q4. What are the main categories of anomaly detection algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b06413f-2f25-4853-979b-09f250e701e1",
   "metadata": {},
   "source": [
    "Statistical Methods:\n",
    "- Use statistical models to identify data points significantly different from the majority of the data.\n",
    "- Examples: Z-score, Gaussian models.\n",
    "\n",
    "Machine Learning Methods:\n",
    "- Utilize supervised and unsupervised learning algorithms to detect anomalies.\n",
    "- Examples: Neural networks, support vector machines, clustering algorithms.\n",
    "\n",
    "Distance-Based Methods:\n",
    "- Measure the distance between data points and identify those far from the majority as anomalies.\n",
    "- Examples: k-nearest neighbors (k-NN), Local Outlier Factor (LOF).\n",
    "\n",
    "Density-Based Methods:\n",
    "- Identify regions of the data space with low density as containing anomalies.\n",
    "- Examples: DBSCAN (Density-Based Spatial Clustering of Applications with Noise), LOF.\n",
    "\n",
    "Model-Based Methods:\n",
    "- Create models representing normal behavior and flag deviations from this model as anomalies.\n",
    "- Examples: Autoencoders, Hidden Markov Models (HMMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e393db-4642-45e0-b496-6611b7832c34",
   "metadata": {},
   "source": [
    "**Q5. What are the main assumptions made by distance-based anomaly detection methods?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d77e2-6163-4889-a951-93ff91de5d85",
   "metadata": {},
   "source": [
    "- Normal Data Points Are Close to Each Other: Normal instances in the dataset are expected to be grouped together in a dense cluster.\n",
    "- Anomalies Are Far from Normal Points: Anomalous data points are assumed to be at a significant distance from the cluster of normal points.\n",
    "- Distance Metric Assumption: The chosen distance metric (e.g., Euclidean distance) accurately reflects the similarity between data points. This assumes the metric appropriately measures the \"closeness\" of data points in the feature space.\n",
    "- Homogeneity of Normal Data: The method assumes that normal data points exhibit similar patterns and deviations from these patterns are considered anomalies.\n",
    "- Uniform Distribution of Data: In some cases, it assumes that normal data points are uniformly distributed, and anomalies deviate significantly from this uniformity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b147d5e-6374-44b9-812e-030665edb657",
   "metadata": {},
   "source": [
    "**Q6. How does the LOF algorithm compute anomaly scores?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2d373-4f19-41e7-9e1e-f66647d014b4",
   "metadata": {},
   "source": [
    "Local Outlier Factor (LOF) computes an anomaly score for each data point based on the local density of its neighborhood compared to the density of its neighbors' neighborhoods. Here's the breakdown:\n",
    "- K-Nearest Neighbors (KNN): LOF first identifies the K-nearest neighbors for each data point.\n",
    "- Local Reachability Density (LRD): For each data point, the LRD is calculated as the reciprocal of the average distance to its K-nearest neighbors. Essentially, it measures how dense the neighborhood is.\n",
    "- Local Outlier Factor (LOF): The LOF score for a data point is the ratio of the average LRD of its K-nearest neighbors to its own LRD. A significantly lower LOF score (compared to its neighbors) indicates that the data point is in a sparse region (far from its neighbors) and hence might be an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9d58ae-c0f7-4135-89c2-1094f1eaebc2",
   "metadata": {},
   "source": [
    "**Q7. What are the key parameters of the Isolation Forest algorithm?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c32dd99-8bec-4ab0-9927-fa2dd5672334",
   "metadata": {},
   "source": [
    "- Number of Trees (n_estimators): This parameter specifies the number of trees in the forest. A higher number of trees generally improves the model's performance but also increases computational cost.\n",
    "- Subsample Size (max_samples): This parameter determines the number of samples to draw from the dataset to train each tree. It is a trade-off between computational efficiency and accuracy. Smaller subsample sizes may result in more efficient training but potentially less accurate models.\n",
    "- Maximum Number of Features (max_features): Specifies the number of features to consider when looking for the best split. Limiting this number can speed up the computation and help prevent overfitting.\n",
    "- Contamination (contamination): An optional parameter that specifies the proportion of outliers in the data. This is used to define the threshold for the decision function. If not set, the algorithm determines the threshold based on the fitted data.\n",
    "- Random Seed (random_state): Ensures reproducibility by controlling the randomness involved in the data sampling and the feature selection processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7f41c-b0fa-4284-94b5-d778c997f6f7",
   "metadata": {},
   "source": [
    "**Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score\n",
    "using KNN with K=10?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53063b-f8e4-4ef2-8c39-a0d483aa2f6c",
   "metadata": {},
   "source": [
    "If a data point p has only 2 neighbors within a radius of 0.5 and we are using k-nearest neighbors (KNN) with K=10, its anomaly score can be calculated as follows:\n",
    "- Determine Neighbors: The data point p only has 2 neighbors within the specified radius of 0.5. For KNN with K=10, we need to find the 10 nearest neighbors of p.\n",
    "- Anomaly Score Calculation: An anomaly score in KNN is often calculated based on the distance to the K-th nearest neighbor. Since p only has 2 neighbors within a close radius and we need 10 neighbors, the distance to the 10th nearest neighbor will be significantly larger.   \n",
    "\n",
    "The anomaly score in this context can be thought of as the inverse of the density around the point. With only 2 close neighbors and needing 10, the point p is in a sparse region, indicating it is likely an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a297d-c53a-4480-8a0f-2d54de8b4552",
   "metadata": {},
   "source": [
    "**Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the\n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path\n",
    "length of the trees?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c9805-46ab-4308-b19c-fdbdeabf339b",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score is based on the average path length $ h(x) $ of a data point $ x $. The anomaly score can be interpreted using the following steps:\n",
    "\n",
    "1. Average Path Length Calculation: \n",
    "   - The average path length $ c(n) $ for a dataset of size $ n $ can be approximated by:\n",
    "     $\n",
    "     c(n) = 2H(n-1) - \\frac{2(n-1)}{n}\n",
    "     $\n",
    "     where $ H(i) $ is the harmonic number, which can be approximated as $ H(i) \\approx \\ln(i) + \\gamma $ (Euler's constant $ \\gamma \\approx 0.57721 $).\n",
    "\n",
    "   - For $ n = 3000 $:\n",
    "     $\n",
    "     c(3000) \\approx 2 \\ln(2999) + 2\\gamma - \\frac{2 \\cdot 2999}{3000} \\approx 2 \\cdot 8.006 + 1.154 - 1.998 \\approx 15.166\n",
    "     $\n",
    "\n",
    "2. Anomaly Score Calculation:\n",
    "   - The anomaly score $ s $ for a data point with an average path length $ h(x) $ is given by:\n",
    "     $\n",
    "     s(x, n) = 2^{-\\frac{h(x)}{c(n)}}\n",
    "     $\n",
    "\n",
    "   - For $ h(x) = 5.0 $ and $ c(3000) \\approx 15.166 $:\n",
    "     $\n",
    "     s(x, 3000) = 2^{-\\frac{5.0}{15.166}} \\approx 2^{-0.3296} \\approx 0.793\n",
    "     $\n",
    "\n",
    "Therefore, the anomaly score for the data point is approximately \\( 0.793 \\). This score indicates that the point is less likely to be an anomaly (since values closer to 1 indicate normality and values closer to 0 indicate anomalies)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
